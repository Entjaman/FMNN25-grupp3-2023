{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "This describes the first bigger programming project in the course, devoted to artificial\n",
    "neural networks.\n",
    "Application: Recognizing handwritten numbers\n",
    "Most people effortlessly recognise these digits as 504192. This ease is deceptive. The\n",
    "difficulty of visual pattern recognition becomes obvious when trying to write a computer\n",
    "program to recognise digits like the above. What seems easy when we do it ourselves\n",
    "suddenly becomes extremely difficult. Simple notions about how we recognise shapes -\n",
    "“a 9 has a loop at the top and a vertical line at the bottom right” - turn out to be not\n",
    "so easy to express algorithmically. If you try to specify such rules, you quickly get lost\n",
    "in a quagmire of exceptions, restrictions and special cases. It seems hopeless.\n",
    "Neural networks approach the problem differently. The idea is to use a large number of\n",
    "handwritten digits, called training examples, and then develop a system that can learn\n",
    "from these training examples. In other words, the neural network uses the examples to\n",
    "automatically derive rules for recognising handwritten digits. In addition, by increasing\n",
    "the number of training examples, the network can learn more about handwriting and\n",
    "thus improve its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "# pickle is for loading the dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Implement a feedforward neural network (as a class) consisting of 3 layers (input,\n",
    "hidden, output layer), where each layer can contain any number of neurons. Use\n",
    "the sigmoid function as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implement the stochastic gradient method (SGD) to train the network. The implementation of the SGD should allow for different mini-batch sizes and different\n",
    "numbers of epochs. An epoch is the complete pass of the training data through\n",
    "the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "Implement the backpropagation algorithm (used in SGD to effectively calculate\n",
    "the derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward NN Class\n",
    "class FeedForwardNeuralNetwork():\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # randn was used to achieve a better initial estimate of the weights and biases by having random values from a \n",
    "        # gaussian distribution. This results in better accuracy    \n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.random.randn(self.hidden_size)\n",
    "        self.weights_output_hidden = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.random.randn(self.output_size)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    # performs the forward pass through the neural network : that is, it does matrix multiplication(np.dot) and adds the\n",
    "    # biases\n",
    "    def forward(self, input_data):\n",
    "        hidden_input = np.dot(input_data, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        output = np.dot(hidden_output, self.weights_output_hidden) + self.bias_output\n",
    "        network_output = self.sigmoid(output)\n",
    "        return network_output, hidden_output\n",
    "    \n",
    "\n",
    "    def train_sgd(self, X_train, Y_train, X_val, Y_val_onehot, learning_rate, batch_size, epochs):\n",
    "        \n",
    "        \"\"\"\n",
    "        X_train : training samples\n",
    "        Y_train : training labels\n",
    "        x_val   : validation samples\n",
    "        Y_val_onehot  : validation labels\n",
    "        learning_rate : step size for the backpropagation\n",
    "        batch_size    : the number of samples processed before the model is updated\n",
    "        epochs        : the number of complete passes through the entire dataset\n",
    "        \n",
    "        \"\"\"\n",
    "        # starting value for the loss\n",
    "        best_val_loss = 100\n",
    "        indices = np.arange(len(X_train))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the dataset so that the batch samples are not consecutive\n",
    "            np.random.shuffle(indices)\n",
    "            total_loss = []\n",
    "\n",
    "            print(f'----------epoch: {epoch}------------')\n",
    "\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                # Create mini-batches\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = Y_train[batch_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                network_output, hidden_output = self.forward(X_batch)\n",
    "\n",
    "                # Backpropagation\n",
    "                # Calculate loss and gradients\n",
    "                # quadratic function (square loss) -- 1/2n is the mean from the formula\n",
    "                loss = np.mean(0.5 * (network_output - y_batch) ** 2, axis=0)\n",
    "                # the loss is averaged over the multiple losses to have a prettier print\n",
    "                total_loss.append(np.mean(loss))  # Average over the mini-batch\n",
    "\n",
    "                error = network_output - y_batch\n",
    "                # calculate the gradient (derivative) of the loss with respect to the output of the neural network's final layer.\n",
    "                d_output = error * network_output * (1 - network_output)\n",
    "                # calculate the gradient of the loss with respect to the hidden layer's activations.\n",
    "                d_hidden = np.dot(d_output, self.weights_output_hidden.T) * hidden_output * (1 - hidden_output)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.weights_output_hidden -= learning_rate * np.dot(hidden_output.T, d_output)\n",
    "                self.bias_output -= learning_rate * np.sum(d_output, axis=0)\n",
    "                self.weights_input_hidden -= learning_rate * np.dot(X_batch.T, d_hidden)\n",
    "                self.bias_hidden -= learning_rate * np.sum(d_hidden, axis=0)\n",
    "                \n",
    "            print(f'mean total loss: {np.mean(total_loss)}')\n",
    "                # Evaluate the model on the validation set\n",
    "                #maybe comment it if we dont want the validation within the training\n",
    " ######################################               \n",
    "#             val_loss, val_accuracy = self.evaluate(X_val, Y_val_onehot)\n",
    "#             print(f\"Epoch {epoch+1}: Validation Loss = {val_loss:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n",
    "    \n",
    "#             # Check for early stopping or other criteria to save the best model\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 woh =  self.weights_output_hidden\n",
    "#                 bo = self.bias_output\n",
    "#                 wih = self.weights_input_hidden\n",
    "#                 bh = self.bias_hidden\n",
    "\n",
    "#         # Update weights to the best weights\n",
    "#         self.weights_output_hidden = woh \n",
    "#         self.bias_output = bo\n",
    "#         self.weights_input_hidden = wih\n",
    "#         self.bias_hidden = bh\n",
    "#############################\n",
    "\n",
    "    def evaluate(self, X_val, Y_val):\n",
    "        num_examples = len(X_val)\n",
    "        # Forward pass on the validation data\n",
    "        val_predictions = self.predict(X_val)\n",
    "        # Calculate the mean loss\n",
    "        val_loss = np.mean(0.5 * (val_predictions - X_val) ** 2, axis=0)\n",
    "\n",
    "        # Convert predicted probabilities to predicted class labels (0-9)\n",
    "        val_predictions_class = np.argmax(val_predictions, axis=1)\n",
    "        \n",
    "        # Convert true labels (one-hot encoded) to true class labels (0-9)\n",
    "        Y_val_class = np.argmax(Y_val, axis=1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        val_accuracy = np.sum(val_predictions_class == Y_val_class) / num_examples\n",
    "\n",
    "        return val_loss, val_accuracy\n",
    "\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        # Perform a forward pass to generate predictions\n",
    "        hidden_input = np.dot(input_data, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        output = np.dot(hidden_output, self.weights_output_hidden) + self.bias_output\n",
    "        network_output = self.sigmoid(output)\n",
    "\n",
    "        # Return the predicted values\n",
    "        return network_output\n",
    "    \n",
    "    # functions to save the weights and biases in a txt file and to load it in the model\n",
    "    def save_weights(self, filename):\n",
    "        \"\"\"\n",
    "        This function saves the weights from the model in order i.e. the weights from the first layer and then second according\n",
    "        to the labels given in the function call(which is before the equal)\n",
    "        The file will have the .npz extension automatically if not written specifically in the filename\n",
    "        \"\"\"\n",
    "        #probably want to save the number of neurons on each layer as well, and restore the model with load(but i thought about it a bit too late)\n",
    "        np.savez(filename, weights_input_hidden=self.weights_input_hidden,\n",
    "                           bias_hidden=self.bias_hidden,\n",
    "                           weights_output_hidden= self.weights_output_hidden,\n",
    "                           bias_output =self.bias_output)\n",
    "    \n",
    "    \n",
    "    def load_weights(self,filename):\n",
    "        \"\"\"\n",
    "        Loads the weights from a file. The filename should have the extension .npz in the name. The weights are loaded\n",
    "        according to the labels which have been used to save it\n",
    "        \n",
    "        \"\"\"\n",
    "        all_weights = np.load(filename)\n",
    "        self.weights_input_hidden = all_weights['weights_input_hidden']\n",
    "        self.bias_hidden = all_weights['bias_hidden']\n",
    "        self.weights_output_hidden = all_weights['weights_output_hidden']\n",
    "        self.bias_output = all_weights['bias_output']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Reading in MNIST data (provided in canvas). The data is separated into training\n",
    "data (50 000), validation data (10 000), and test data (10 000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data from mnist.pkl\n",
    "with open('mnist.pkl/mnist.pkl', 'rb') as f:\n",
    "    mnist_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "# Extract training, validation, and test sets\n",
    "train_data, valid_data, test_data = mnist_data\n",
    "\n",
    "# Unpack the data into inputs and labels\n",
    "X_train, Y_train = train_data\n",
    "X_val, Y_val = valid_data\n",
    "X_test, Y_test = test_data\n",
    "\n",
    "# Convert inputs to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(Y_val)\n",
    "Y_test = np.array(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f = open('test', \"w\")\n",
    "np.savez('test', X_train=X_train,X_val=X_val,test=X_test)\n",
    "X_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.load('test.npz')\n",
    "# if a[1][:].all() == X_test.all():\n",
    "#     print(\"da\")\n",
    "# else:\n",
    "#     print(\"nu\")\n",
    "a['test'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onehot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 10), (50000, 784))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding - maps the numbers for all the dataset to vectors with 1s and 0s \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "Y_train_onehot = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
    "Y_val_onehot = encoder.fit_transform(Y_val.reshape(-1, 1))\n",
    "\n",
    "np.shape(Y_train_onehot), np.shape(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Train and test the accuracy of the network for the following parameters:\n",
    "\n",
    "• Input layer with 784 + 1 neurons\n",
    "\n",
    "• hidden layer with 30 + 1 neurons\n",
    "\n",
    "• Output layer with 10 neurons\n",
    "\n",
    "As loss function use the quadratic function (square loss),\n",
    "where (x, y) is a pair of training data, n the amount of used training data, and hw\n",
    "represents the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch: 0------------\n",
      "mean total loss: 0.022261697737608815\n",
      "----------epoch: 1------------\n",
      "mean total loss: 0.009607812493412336\n",
      "----------epoch: 2------------\n",
      "mean total loss: 0.007826324753374305\n",
      "----------epoch: 3------------\n",
      "mean total loss: 0.006956604445381115\n",
      "----------epoch: 4------------\n",
      "mean total loss: 0.006349415583317877\n",
      "----------epoch: 5------------\n",
      "mean total loss: 0.005914042392421485\n",
      "----------epoch: 6------------\n",
      "mean total loss: 0.005550460508692292\n",
      "----------epoch: 7------------\n",
      "mean total loss: 0.005251495453296409\n",
      "----------epoch: 8------------\n",
      "mean total loss: 0.00499551226065673\n",
      "----------epoch: 9------------\n",
      "mean total loss: 0.004786812226566876\n",
      "----------epoch: 10------------\n",
      "mean total loss: 0.004598412307938072\n",
      "----------epoch: 11------------\n",
      "mean total loss: 0.004423527087225509\n",
      "----------epoch: 12------------\n",
      "mean total loss: 0.004291259186388712\n",
      "----------epoch: 13------------\n",
      "mean total loss: 0.004155781945129261\n",
      "----------epoch: 14------------\n",
      "mean total loss: 0.004034389119951228\n",
      "----------epoch: 15------------\n",
      "mean total loss: 0.003930459924498878\n",
      "----------epoch: 16------------\n",
      "mean total loss: 0.003825977428480915\n",
      "----------epoch: 17------------\n",
      "mean total loss: 0.0037509415354421297\n",
      "----------epoch: 18------------\n",
      "mean total loss: 0.0036669978504144654\n",
      "----------epoch: 19------------\n",
      "mean total loss: 0.0035777382977842764\n",
      "----------epoch: 20------------\n",
      "mean total loss: 0.003517251820186557\n",
      "----------epoch: 21------------\n",
      "mean total loss: 0.0034355493409392365\n",
      "----------epoch: 22------------\n",
      "mean total loss: 0.0033855259011690555\n",
      "----------epoch: 23------------\n",
      "mean total loss: 0.003316412024447257\n",
      "----------epoch: 24------------\n",
      "mean total loss: 0.003254456140903307\n",
      "----------epoch: 25------------\n",
      "mean total loss: 0.00320518571753088\n",
      "----------epoch: 26------------\n",
      "mean total loss: 0.0031433033943183567\n",
      "----------epoch: 27------------\n",
      "mean total loss: 0.003097982566307675\n",
      "----------epoch: 28------------\n",
      "mean total loss: 0.003047135080744109\n",
      "----------epoch: 29------------\n",
      "mean total loss: 0.003002394310594811\n",
      "----------epoch: 30------------\n",
      "mean total loss: 0.00295468363638634\n",
      "----------epoch: 31------------\n",
      "mean total loss: 0.00291995360913367\n",
      "----------epoch: 32------------\n",
      "mean total loss: 0.002873687747982533\n",
      "----------epoch: 33------------\n",
      "mean total loss: 0.0028369255934141335\n",
      "----------epoch: 34------------\n",
      "mean total loss: 0.002811195392517218\n",
      "----------epoch: 35------------\n",
      "mean total loss: 0.002769737781700555\n",
      "----------epoch: 36------------\n",
      "mean total loss: 0.0027426288429355863\n",
      "----------epoch: 37------------\n",
      "mean total loss: 0.0027043279321467646\n",
      "----------epoch: 38------------\n",
      "mean total loss: 0.0026762895315189583\n",
      "----------epoch: 39------------\n",
      "mean total loss: 0.002638611241929589\n",
      "----------epoch: 40------------\n",
      "mean total loss: 0.002606644975255439\n",
      "----------epoch: 41------------\n",
      "mean total loss: 0.0025797707433368885\n",
      "----------epoch: 42------------\n",
      "mean total loss: 0.0025518672951293253\n",
      "----------epoch: 43------------\n",
      "mean total loss: 0.00252200835948428\n",
      "----------epoch: 44------------\n",
      "mean total loss: 0.0024860488872971544\n",
      "----------epoch: 45------------\n",
      "mean total loss: 0.0024728583028629375\n",
      "----------epoch: 46------------\n",
      "mean total loss: 0.0024507520525422627\n",
      "----------epoch: 47------------\n",
      "mean total loss: 0.002415856043986635\n",
      "----------epoch: 48------------\n",
      "mean total loss: 0.002388354943799281\n",
      "----------epoch: 49------------\n",
      "mean total loss: 0.002370056984356885\n",
      "----------epoch: 50------------\n",
      "mean total loss: 0.0023445761939546914\n",
      "----------epoch: 51------------\n",
      "mean total loss: 0.002326714372393107\n",
      "----------epoch: 52------------\n",
      "mean total loss: 0.0023029404919461794\n",
      "----------epoch: 53------------\n",
      "mean total loss: 0.002283364483763982\n",
      "----------epoch: 54------------\n",
      "mean total loss: 0.0022601611129668907\n",
      "----------epoch: 55------------\n",
      "mean total loss: 0.0022457392990749233\n",
      "----------epoch: 56------------\n",
      "mean total loss: 0.0022230056813305403\n",
      "----------epoch: 57------------\n",
      "mean total loss: 0.002202791109979783\n",
      "----------epoch: 58------------\n",
      "mean total loss: 0.0021842409691358805\n",
      "----------epoch: 59------------\n",
      "mean total loss: 0.002159919441922183\n",
      "----------epoch: 60------------\n",
      "mean total loss: 0.002149933333791349\n",
      "----------epoch: 61------------\n",
      "mean total loss: 0.0021364265882154424\n",
      "----------epoch: 62------------\n",
      "mean total loss: 0.0021038946454583437\n",
      "----------epoch: 63------------\n",
      "mean total loss: 0.002098942812074477\n",
      "----------epoch: 64------------\n",
      "mean total loss: 0.0020943565071134397\n",
      "----------epoch: 65------------\n",
      "mean total loss: 0.0020719726170530927\n",
      "----------epoch: 66------------\n",
      "mean total loss: 0.002057476038609386\n",
      "----------epoch: 67------------\n",
      "mean total loss: 0.0020435076551090763\n",
      "----------epoch: 68------------\n",
      "mean total loss: 0.0020180590151037555\n",
      "----------epoch: 69------------\n",
      "mean total loss: 0.0020134331801473756\n",
      "----------epoch: 70------------\n",
      "mean total loss: 0.0020063402093551154\n",
      "----------epoch: 71------------\n",
      "mean total loss: 0.001987663484790087\n",
      "----------epoch: 72------------\n",
      "mean total loss: 0.0019710517204434125\n",
      "----------epoch: 73------------\n",
      "mean total loss: 0.0019583824235061722\n",
      "----------epoch: 74------------\n",
      "mean total loss: 0.0019413146145160337\n",
      "----------epoch: 75------------\n",
      "mean total loss: 0.0019345561426737635\n",
      "----------epoch: 76------------\n",
      "mean total loss: 0.0019277206050142715\n",
      "----------epoch: 77------------\n",
      "mean total loss: 0.0019091422237469638\n",
      "----------epoch: 78------------\n",
      "mean total loss: 0.0018957980903174197\n",
      "----------epoch: 79------------\n",
      "mean total loss: 0.0018797129920476783\n",
      "----------epoch: 80------------\n",
      "mean total loss: 0.0018708795708730537\n",
      "----------epoch: 81------------\n",
      "mean total loss: 0.0018626468016777465\n",
      "----------epoch: 82------------\n",
      "mean total loss: 0.0018500039690081357\n",
      "----------epoch: 83------------\n",
      "mean total loss: 0.0018430391845352054\n",
      "----------epoch: 84------------\n",
      "mean total loss: 0.0018206008756634572\n",
      "----------epoch: 85------------\n",
      "mean total loss: 0.0018215641101623607\n",
      "----------epoch: 86------------\n",
      "mean total loss: 0.0018026925552854086\n",
      "----------epoch: 87------------\n",
      "mean total loss: 0.0017961900828775447\n",
      "----------epoch: 88------------\n",
      "mean total loss: 0.0017818852801428904\n",
      "----------epoch: 89------------\n",
      "mean total loss: 0.0017754560591630596\n",
      "----------epoch: 90------------\n",
      "mean total loss: 0.001771913608063939\n",
      "----------epoch: 91------------\n",
      "mean total loss: 0.0017578031596898332\n",
      "----------epoch: 92------------\n",
      "mean total loss: 0.0017468368817492037\n",
      "----------epoch: 93------------\n",
      "mean total loss: 0.0017348337614709528\n",
      "----------epoch: 94------------\n",
      "mean total loss: 0.0017303285700719382\n",
      "----------epoch: 95------------\n",
      "mean total loss: 0.0017207468315488933\n",
      "----------epoch: 96------------\n",
      "mean total loss: 0.0017058258195789234\n",
      "----------epoch: 97------------\n",
      "mean total loss: 0.0016963227930774435\n",
      "----------epoch: 98------------\n",
      "mean total loss: 0.0016922910937195823\n",
      "----------epoch: 99------------\n",
      "mean total loss: 0.0016866994125895446\n"
     ]
    }
   ],
   "source": [
    "nn = FeedForwardNeuralNetwork(784, 30, 10)\n",
    "\n",
    "nn.train_sgd(X_train, Y_train_onehot, X_val, Y_val_onehot, 0.1, 1, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "# Convert to labels (0-9)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7, 2, 1, ..., 4, 5, 6], dtype=int64),\n",
       " array([7, 2, 1, ..., 4, 5, 6], dtype=int64))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9491"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "nn.save_weights('model_weights')\n",
    "# not sure if this is the best or most elegant to do it -- probably want to integrate it somehow in the function\n",
    "saved_nn = FeedForwardNeuralNetwork(784, 30, 10)\n",
    "saved_nn.load_weights('model_weights.npz')\n",
    "y_pred = nn.predict(X_test)\n",
    "# Convert to labels (0-9)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Print an output of the learning success per epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
