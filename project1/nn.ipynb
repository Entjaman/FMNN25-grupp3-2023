{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "This describes the first bigger programming project in the course, devoted to artificial\n",
    "neural networks.\n",
    "Application: Recognizing handwritten numbers\n",
    "Most people effortlessly recognise these digits as 504192. This ease is deceptive. The\n",
    "difficulty of visual pattern recognition becomes obvious when trying to write a computer\n",
    "program to recognise digits like the above. What seems easy when we do it ourselves\n",
    "suddenly becomes extremely difficult. Simple notions about how we recognise shapes -\n",
    "“a 9 has a loop at the top and a vertical line at the bottom right” - turn out to be not\n",
    "so easy to express algorithmically. If you try to specify such rules, you quickly get lost\n",
    "in a quagmire of exceptions, restrictions and special cases. It seems hopeless.\n",
    "Neural networks approach the problem differently. The idea is to use a large number of\n",
    "handwritten digits, called training examples, and then develop a system that can learn\n",
    "from these training examples. In other words, the neural network uses the examples to\n",
    "automatically derive rules for recognising handwritten digits. In addition, by increasing\n",
    "the number of training examples, the network can learn more about handwriting and\n",
    "thus improve its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Implement a feedforward neural network (as a class) consisting of 3 layers (input,\n",
    "hidden, output layer), where each layer can contain any number of neurons. Use\n",
    "the sigmoid function as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implement the stochastic gradient method (SGD) to train the network. The implementation of the SGD should allow for different mini-batch sizes and different\n",
    "numbers of epochs. An epoch is the complete pass of the training data through\n",
    "the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "Implement the backpropagation algorithm (used in SGD to effectively calculate\n",
    "the derivative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward NN Class\n",
    "class FeedForwardNeuralNetwork():\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "            \n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias_hidden = np.random.randn(self.hidden_size)\n",
    "        self.weights_output_hidden = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias_output = np.random.randn(self.output_size)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        hidden_input = np.dot(input_data, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "\n",
    "        output = np.dot(hidden_output, self.weights_output_hidden) + self.bias_output\n",
    "        network_output = self.sigmoid(output)\n",
    "\n",
    "        return network_output\n",
    "    \n",
    "    def train_sgd(self, X_train, y_train, X_val, Y_val_onehot, learning_rate, batch_size, epochs):\n",
    "        best_val_loss = 100\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the dataset\n",
    "            indices = np.arange(len(X_train))\n",
    "            np.random.shuffle(indices)\n",
    "            total_loss = []\n",
    "\n",
    "            print(f'----------epoch: {epoch}------------')\n",
    "\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                # Create mini-batches\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train[batch_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                hidden_input = np.dot(X_batch, self.weights_input_hidden) + self.bias_hidden\n",
    "                hidden_output = self.sigmoid(hidden_input)\n",
    "                output = np.dot(hidden_output, self.weights_output_hidden) + self.bias_output\n",
    "                network_output = self.sigmoid(output)\n",
    "\n",
    "                # Backpropagation\n",
    "                # Calculate loss and gradients\n",
    "                # quadratic function (square loss)\n",
    "                loss = np.mean(0.5 * (network_output - y_batch) ** 2, axis=0)\n",
    "                total_loss.append(np.mean(loss))  # Average over the mini-batch\n",
    "\n",
    "                error = network_output - y_batch\n",
    "                # calculate the gradient (derivative) of the loss with respect to the output of the neural network's final layer.\n",
    "                d_output = error * network_output * (1 - network_output)\n",
    "                # calculate the gradient of the loss with respect to the hidden layer's activations.\n",
    "                d_hidden = np.dot(d_output, self.weights_output_hidden.T) * hidden_output * (1 - hidden_output)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.weights_output_hidden -= learning_rate * np.dot(hidden_output.T, d_output)\n",
    "                self.bias_output -= learning_rate * np.sum(d_output, axis=0)\n",
    "                self.weights_input_hidden -= learning_rate * np.dot(X_batch.T, d_hidden)\n",
    "                self.bias_hidden -= learning_rate * np.sum(d_hidden, axis=0)\n",
    "                \n",
    "            print(f'mean total loss: {np.mean(total_loss)}')\n",
    "                # Evaluate the model on the validation set\n",
    "            val_loss, val_accuracy = self.evaluate(X_val, Y_val_onehot)\n",
    "            print(f\"Epoch {epoch+1}: Validation Loss = {val_loss:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n",
    "    \n",
    "            # Check for early stopping or other criteria to save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                woh =  self.weights_output_hidden\n",
    "                bo = self.bias_output\n",
    "                wih = self.weights_input_hidden\n",
    "                bh = self.bias_hidden\n",
    "\n",
    "        # Update weights to the best weights\n",
    "        self.weights_output_hidden = woh \n",
    "        self.bias_output = bo\n",
    "        self.weights_input_hidden = wih\n",
    "        self.bias_hidden = bh\n",
    "\n",
    "    def evaluate(self, X_val, Y_val):\n",
    "        num_examples = len(X_val)\n",
    "        # Forward pass on the validation data\n",
    "        val_predictions = self.predict(X_val)\n",
    "        # Calculate the categorical cross-entropy loss\n",
    "        val_loss = -np.sum(Y_val * np.log(val_predictions + 1e-15)) / num_examples\n",
    "\n",
    "        # Convert predicted probabilities to predicted class labels (0-9)\n",
    "        val_predictions_class = np.argmax(val_predictions, axis=1)\n",
    "        \n",
    "        # Convert true labels (one-hot encoded) to true class labels (0-9)\n",
    "        Y_val_class = np.argmax(Y_val, axis=1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        val_accuracy = np.sum(val_predictions_class == Y_val_class) / num_examples\n",
    "\n",
    "        return val_loss, val_accuracy\n",
    "\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        # Perform a forward pass to generate predictions\n",
    "        hidden_input = np.dot(input_data, self.weights_input_hidden) + self.bias_hidden\n",
    "        hidden_output = self.sigmoid(hidden_input)\n",
    "        output = np.dot(hidden_output, self.weights_output_hidden) + self.bias_output\n",
    "        network_output = self.sigmoid(output)\n",
    "\n",
    "        # Return the predicted values\n",
    "        return network_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Reading in MNIST data (provided in canvas). The data is separated into training\n",
    "data (50 000), validation data (10 000), and test data (10 000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data from mnist.pkl\n",
    "with open('mnist.pkl', 'rb') as f:\n",
    "    mnist_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "# Extract training, validation, and test sets\n",
    "train_data, valid_data, test_data = mnist_data\n",
    "\n",
    "# Unpack the data into inputs and labels\n",
    "X_train, Y_train = train_data\n",
    "X_val, Y_val = valid_data\n",
    "X_test, Y_test = test_data\n",
    "\n",
    "# Convert inputs to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(Y_val)\n",
    "Y_test = np.array(Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onehot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 10), (50000, 784))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "Y_train_onehot = encoder.fit_transform(Y_train.reshape(-1, 1))\n",
    "Y_val_onehot = encoder.fit_transform(Y_val.reshape(-1, 1))\n",
    "\n",
    "np.shape(Y_train_onehot), np.shape(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "Train and test the accuracy of the network for the following parameters:\n",
    "\n",
    "• Input layer with 784 + 1 neurons\n",
    "\n",
    "• hidden layer with 30 + 1 neurons\n",
    "\n",
    "• Output layer with 10 neurons\n",
    "\n",
    "As loss function use the quadratic function (square loss),\n",
    "where (x, y) is a pair of training data, n the amount of used training data, and hw\n",
    "represents the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch: 0------------\n",
      "mean total loss: 0.02643615672831189\n",
      "Epoch 1: Validation Loss = 3.2324, Validation Accuracy = 0.6656\n",
      "----------epoch: 1------------\n",
      "mean total loss: 0.018135396144793193\n",
      "Epoch 2: Validation Loss = 2.6348, Validation Accuracy = 0.7541\n",
      "----------epoch: 2------------\n",
      "mean total loss: 0.015555680781183162\n",
      "Epoch 3: Validation Loss = 2.5913, Validation Accuracy = 0.7609\n",
      "----------epoch: 3------------\n",
      "mean total loss: 0.014774703651921604\n",
      "Epoch 4: Validation Loss = 2.5630, Validation Accuracy = 0.7649\n",
      "----------epoch: 4------------\n",
      "mean total loss: 0.014290993677797273\n",
      "Epoch 5: Validation Loss = 2.5704, Validation Accuracy = 0.7669\n",
      "----------epoch: 5------------\n",
      "mean total loss: 0.013937079170021054\n",
      "Epoch 6: Validation Loss = 2.5896, Validation Accuracy = 0.7671\n",
      "----------epoch: 6------------\n",
      "mean total loss: 0.013657118741604244\n",
      "Epoch 7: Validation Loss = 2.5102, Validation Accuracy = 0.7701\n",
      "----------epoch: 7------------\n",
      "mean total loss: 0.013448852778359895\n",
      "Epoch 8: Validation Loss = 2.4652, Validation Accuracy = 0.7717\n",
      "----------epoch: 8------------\n",
      "mean total loss: 0.013271761338877853\n",
      "Epoch 9: Validation Loss = 2.4598, Validation Accuracy = 0.7744\n",
      "----------epoch: 9------------\n",
      "mean total loss: 0.013134628321935668\n",
      "Epoch 10: Validation Loss = 2.3922, Validation Accuracy = 0.7754\n",
      "----------epoch: 10------------\n",
      "mean total loss: 0.013001033398638898\n",
      "Epoch 11: Validation Loss = 2.3644, Validation Accuracy = 0.7771\n",
      "----------epoch: 11------------\n",
      "mean total loss: 0.012884416004366914\n",
      "Epoch 12: Validation Loss = 2.2993, Validation Accuracy = 0.7776\n",
      "----------epoch: 12------------\n",
      "mean total loss: 0.012766620869209369\n",
      "Epoch 13: Validation Loss = 2.2382, Validation Accuracy = 0.7772\n",
      "----------epoch: 13------------\n",
      "mean total loss: 0.012663506145344283\n",
      "Epoch 14: Validation Loss = 2.0464, Validation Accuracy = 0.7807\n",
      "----------epoch: 14------------\n",
      "mean total loss: 0.011404051307834173\n",
      "Epoch 15: Validation Loss = 1.2486, Validation Accuracy = 0.8394\n",
      "----------epoch: 15------------\n",
      "mean total loss: 0.009330479468360608\n",
      "Epoch 16: Validation Loss = 0.3423, Validation Accuracy = 0.9336\n",
      "----------epoch: 16------------\n",
      "mean total loss: 0.00545600663993336\n",
      "Epoch 17: Validation Loss = 0.3025, Validation Accuracy = 0.9396\n",
      "----------epoch: 17------------\n",
      "mean total loss: 0.004995644114479433\n",
      "Epoch 18: Validation Loss = 0.3096, Validation Accuracy = 0.9416\n",
      "----------epoch: 18------------\n",
      "mean total loss: 0.004738064644361488\n",
      "Epoch 19: Validation Loss = 0.3189, Validation Accuracy = 0.9437\n",
      "----------epoch: 19------------\n",
      "mean total loss: 0.004525998223908762\n",
      "Epoch 20: Validation Loss = 0.2930, Validation Accuracy = 0.9445\n",
      "----------epoch: 20------------\n",
      "mean total loss: 0.00435754918927142\n",
      "Epoch 21: Validation Loss = 0.3014, Validation Accuracy = 0.9437\n",
      "----------epoch: 21------------\n",
      "mean total loss: 0.004254236064973996\n",
      "Epoch 22: Validation Loss = 0.3025, Validation Accuracy = 0.9437\n",
      "----------epoch: 22------------\n",
      "mean total loss: 0.0041295390152484585\n",
      "Epoch 23: Validation Loss = 0.2879, Validation Accuracy = 0.9458\n",
      "----------epoch: 23------------\n",
      "mean total loss: 0.004010688092850264\n",
      "Epoch 24: Validation Loss = 0.2960, Validation Accuracy = 0.9454\n",
      "----------epoch: 24------------\n",
      "mean total loss: 0.003916625881627525\n",
      "Epoch 25: Validation Loss = 0.2913, Validation Accuracy = 0.9458\n",
      "----------epoch: 25------------\n",
      "mean total loss: 0.0038346637787867784\n",
      "Epoch 26: Validation Loss = 0.3137, Validation Accuracy = 0.9451\n",
      "----------epoch: 26------------\n",
      "mean total loss: 0.003738148394161231\n",
      "Epoch 27: Validation Loss = 0.3032, Validation Accuracy = 0.9467\n",
      "----------epoch: 27------------\n",
      "mean total loss: 0.0036723846960402943\n",
      "Epoch 28: Validation Loss = 0.2802, Validation Accuracy = 0.9477\n",
      "----------epoch: 28------------\n",
      "mean total loss: 0.003592128564761249\n",
      "Epoch 29: Validation Loss = 0.3011, Validation Accuracy = 0.9486\n",
      "----------epoch: 29------------\n",
      "mean total loss: 0.0035239201652341166\n",
      "Epoch 30: Validation Loss = 0.2965, Validation Accuracy = 0.9472\n",
      "----------epoch: 30------------\n",
      "mean total loss: 0.00347106696743762\n",
      "Epoch 31: Validation Loss = 0.2943, Validation Accuracy = 0.9474\n",
      "----------epoch: 31------------\n",
      "mean total loss: 0.0034041864687890445\n",
      "Epoch 32: Validation Loss = 0.2868, Validation Accuracy = 0.9490\n",
      "----------epoch: 32------------\n",
      "mean total loss: 0.0033461072101479464\n",
      "Epoch 33: Validation Loss = 0.2998, Validation Accuracy = 0.9480\n",
      "----------epoch: 33------------\n",
      "mean total loss: 0.003295968825093108\n",
      "Epoch 34: Validation Loss = 0.2929, Validation Accuracy = 0.9481\n",
      "----------epoch: 34------------\n",
      "mean total loss: 0.0032442161112809227\n",
      "Epoch 35: Validation Loss = 0.3086, Validation Accuracy = 0.9474\n",
      "----------epoch: 35------------\n",
      "mean total loss: 0.003203207480354367\n",
      "Epoch 36: Validation Loss = 0.3068, Validation Accuracy = 0.9489\n",
      "----------epoch: 36------------\n",
      "mean total loss: 0.003155540111179728\n",
      "Epoch 37: Validation Loss = 0.2956, Validation Accuracy = 0.9499\n",
      "----------epoch: 37------------\n",
      "mean total loss: 0.0031161480450669263\n",
      "Epoch 38: Validation Loss = 0.2964, Validation Accuracy = 0.9497\n",
      "----------epoch: 38------------\n",
      "mean total loss: 0.0030500952557546385\n",
      "Epoch 39: Validation Loss = 0.2984, Validation Accuracy = 0.9491\n",
      "----------epoch: 39------------\n",
      "mean total loss: 0.0030174539301020945\n",
      "Epoch 40: Validation Loss = 0.3000, Validation Accuracy = 0.9493\n",
      "----------epoch: 40------------\n",
      "mean total loss: 0.002985466482711054\n",
      "Epoch 41: Validation Loss = 0.3111, Validation Accuracy = 0.9507\n",
      "----------epoch: 41------------\n",
      "mean total loss: 0.002945836903291153\n",
      "Epoch 42: Validation Loss = 0.2981, Validation Accuracy = 0.9485\n",
      "----------epoch: 42------------\n",
      "mean total loss: 0.002912126970539909\n",
      "Epoch 43: Validation Loss = 0.3112, Validation Accuracy = 0.9488\n",
      "----------epoch: 43------------\n",
      "mean total loss: 0.0028793372455731842\n",
      "Epoch 44: Validation Loss = 0.3035, Validation Accuracy = 0.9506\n",
      "----------epoch: 44------------\n",
      "mean total loss: 0.002838023298953732\n",
      "Epoch 45: Validation Loss = 0.3023, Validation Accuracy = 0.9502\n",
      "----------epoch: 45------------\n",
      "mean total loss: 0.00280805704874441\n",
      "Epoch 46: Validation Loss = 0.3164, Validation Accuracy = 0.9500\n",
      "----------epoch: 46------------\n",
      "mean total loss: 0.002777670993086193\n",
      "Epoch 47: Validation Loss = 0.3034, Validation Accuracy = 0.9496\n",
      "----------epoch: 47------------\n",
      "mean total loss: 0.0027534071985814957\n",
      "Epoch 48: Validation Loss = 0.3058, Validation Accuracy = 0.9501\n",
      "----------epoch: 48------------\n",
      "mean total loss: 0.0027252172019473823\n",
      "Epoch 49: Validation Loss = 0.3103, Validation Accuracy = 0.9508\n",
      "----------epoch: 49------------\n",
      "mean total loss: 0.0026905068194122174\n",
      "Epoch 50: Validation Loss = 0.3135, Validation Accuracy = 0.9510\n",
      "----------epoch: 50------------\n",
      "mean total loss: 0.002668620180394117\n",
      "Epoch 51: Validation Loss = 0.3144, Validation Accuracy = 0.9498\n",
      "----------epoch: 51------------\n",
      "mean total loss: 0.002628449845475664\n",
      "Epoch 52: Validation Loss = 0.3187, Validation Accuracy = 0.9497\n",
      "----------epoch: 52------------\n",
      "mean total loss: 0.002604799472672627\n",
      "Epoch 53: Validation Loss = 0.3246, Validation Accuracy = 0.9510\n",
      "----------epoch: 53------------\n",
      "mean total loss: 0.0025895081802618258\n",
      "Epoch 54: Validation Loss = 0.3216, Validation Accuracy = 0.9504\n",
      "----------epoch: 54------------\n",
      "mean total loss: 0.002566321401347653\n",
      "Epoch 55: Validation Loss = 0.3125, Validation Accuracy = 0.9499\n",
      "----------epoch: 55------------\n",
      "mean total loss: 0.0025454116053822344\n",
      "Epoch 56: Validation Loss = 0.3247, Validation Accuracy = 0.9520\n",
      "----------epoch: 56------------\n",
      "mean total loss: 0.002522775013545237\n",
      "Epoch 57: Validation Loss = 0.3079, Validation Accuracy = 0.9506\n",
      "----------epoch: 57------------\n",
      "mean total loss: 0.0025042636148783545\n",
      "Epoch 58: Validation Loss = 0.3102, Validation Accuracy = 0.9503\n",
      "----------epoch: 58------------\n",
      "mean total loss: 0.0024834015113239685\n",
      "Epoch 59: Validation Loss = 0.3310, Validation Accuracy = 0.9510\n",
      "----------epoch: 59------------\n",
      "mean total loss: 0.002454562904365226\n",
      "Epoch 60: Validation Loss = 0.3235, Validation Accuracy = 0.9506\n",
      "----------epoch: 60------------\n",
      "mean total loss: 0.0024350183827466063\n",
      "Epoch 61: Validation Loss = 0.3268, Validation Accuracy = 0.9502\n",
      "----------epoch: 61------------\n",
      "mean total loss: 0.0024279998009317477\n",
      "Epoch 62: Validation Loss = 0.3277, Validation Accuracy = 0.9504\n",
      "----------epoch: 62------------\n",
      "mean total loss: 0.0024053834684302634\n",
      "Epoch 63: Validation Loss = 0.3278, Validation Accuracy = 0.9500\n",
      "----------epoch: 63------------\n",
      "mean total loss: 0.0023818656334176455\n",
      "Epoch 64: Validation Loss = 0.3383, Validation Accuracy = 0.9512\n",
      "----------epoch: 64------------\n",
      "mean total loss: 0.0023690388145319523\n",
      "Epoch 65: Validation Loss = 0.3403, Validation Accuracy = 0.9498\n",
      "----------epoch: 65------------\n",
      "mean total loss: 0.002352273501415879\n",
      "Epoch 66: Validation Loss = 0.3403, Validation Accuracy = 0.9512\n",
      "----------epoch: 66------------\n",
      "mean total loss: 0.0023376366174423764\n",
      "Epoch 67: Validation Loss = 0.3300, Validation Accuracy = 0.9508\n",
      "----------epoch: 67------------\n",
      "mean total loss: 0.0023184398183773523\n",
      "Epoch 68: Validation Loss = 0.3299, Validation Accuracy = 0.9519\n",
      "----------epoch: 68------------\n",
      "mean total loss: 0.0023024931262812165\n",
      "Epoch 69: Validation Loss = 0.3540, Validation Accuracy = 0.9497\n",
      "----------epoch: 69------------\n",
      "mean total loss: 0.0022871496400214907\n",
      "Epoch 70: Validation Loss = 0.3351, Validation Accuracy = 0.9509\n",
      "----------epoch: 70------------\n",
      "mean total loss: 0.0022719215256155833\n",
      "Epoch 71: Validation Loss = 0.3393, Validation Accuracy = 0.9505\n",
      "----------epoch: 71------------\n",
      "mean total loss: 0.0022542659571856694\n",
      "Epoch 72: Validation Loss = 0.3337, Validation Accuracy = 0.9510\n",
      "----------epoch: 72------------\n",
      "mean total loss: 0.0022417089651263493\n",
      "Epoch 73: Validation Loss = 0.3334, Validation Accuracy = 0.9513\n",
      "----------epoch: 73------------\n",
      "mean total loss: 0.0022203261741926953\n",
      "Epoch 74: Validation Loss = 0.3522, Validation Accuracy = 0.9508\n",
      "----------epoch: 74------------\n",
      "mean total loss: 0.0022149342756212914\n",
      "Epoch 75: Validation Loss = 0.3452, Validation Accuracy = 0.9498\n",
      "----------epoch: 75------------\n",
      "mean total loss: 0.0022078787431056754\n",
      "Epoch 76: Validation Loss = 0.3500, Validation Accuracy = 0.9514\n",
      "----------epoch: 76------------\n",
      "mean total loss: 0.002183894631534848\n",
      "Epoch 77: Validation Loss = 0.3559, Validation Accuracy = 0.9510\n",
      "----------epoch: 77------------\n",
      "mean total loss: 0.0021773734590285355\n",
      "Epoch 78: Validation Loss = 0.3443, Validation Accuracy = 0.9517\n",
      "----------epoch: 78------------\n",
      "mean total loss: 0.0021542437760207578\n",
      "Epoch 79: Validation Loss = 0.3391, Validation Accuracy = 0.9513\n",
      "----------epoch: 79------------\n",
      "mean total loss: 0.002134582848885723\n",
      "Epoch 80: Validation Loss = 0.3427, Validation Accuracy = 0.9514\n",
      "----------epoch: 80------------\n",
      "mean total loss: 0.002129684517116287\n",
      "Epoch 81: Validation Loss = 0.3433, Validation Accuracy = 0.9516\n",
      "----------epoch: 81------------\n",
      "mean total loss: 0.0021204402426721\n",
      "Epoch 82: Validation Loss = 0.3462, Validation Accuracy = 0.9516\n",
      "----------epoch: 82------------\n",
      "mean total loss: 0.002109711286249238\n",
      "Epoch 83: Validation Loss = 0.3579, Validation Accuracy = 0.9515\n",
      "----------epoch: 83------------\n",
      "mean total loss: 0.0020881268135655214\n",
      "Epoch 84: Validation Loss = 0.3542, Validation Accuracy = 0.9510\n",
      "----------epoch: 84------------\n",
      "mean total loss: 0.002078006837408139\n",
      "Epoch 85: Validation Loss = 0.3543, Validation Accuracy = 0.9507\n",
      "----------epoch: 85------------\n",
      "mean total loss: 0.0020618826937611086\n",
      "Epoch 86: Validation Loss = 0.3555, Validation Accuracy = 0.9509\n",
      "----------epoch: 86------------\n",
      "mean total loss: 0.0020557389116382407\n",
      "Epoch 87: Validation Loss = 0.3512, Validation Accuracy = 0.9513\n",
      "----------epoch: 87------------\n",
      "mean total loss: 0.0020487184395616446\n",
      "Epoch 88: Validation Loss = 0.3559, Validation Accuracy = 0.9504\n",
      "----------epoch: 88------------\n",
      "mean total loss: 0.0020243527852397227\n",
      "Epoch 89: Validation Loss = 0.3552, Validation Accuracy = 0.9511\n",
      "----------epoch: 89------------\n",
      "mean total loss: 0.0020389062413710642\n",
      "Epoch 90: Validation Loss = 0.3483, Validation Accuracy = 0.9511\n",
      "----------epoch: 90------------\n",
      "mean total loss: 0.002000600658971835\n",
      "Epoch 91: Validation Loss = 0.3552, Validation Accuracy = 0.9528\n",
      "----------epoch: 91------------\n",
      "mean total loss: 0.0019961617601757815\n",
      "Epoch 92: Validation Loss = 0.3637, Validation Accuracy = 0.9515\n",
      "----------epoch: 92------------\n",
      "mean total loss: 0.0019901301296508904\n",
      "Epoch 93: Validation Loss = 0.3669, Validation Accuracy = 0.9508\n",
      "----------epoch: 93------------\n",
      "mean total loss: 0.0019848845451599273\n",
      "Epoch 94: Validation Loss = 0.3586, Validation Accuracy = 0.9512\n",
      "----------epoch: 94------------\n",
      "mean total loss: 0.001965016585689507\n",
      "Epoch 95: Validation Loss = 0.3685, Validation Accuracy = 0.9502\n",
      "----------epoch: 95------------\n",
      "mean total loss: 0.0019575319089166427\n",
      "Epoch 96: Validation Loss = 0.3692, Validation Accuracy = 0.9516\n",
      "----------epoch: 96------------\n",
      "mean total loss: 0.001962146858038829\n",
      "Epoch 97: Validation Loss = 0.3620, Validation Accuracy = 0.9512\n",
      "----------epoch: 97------------\n",
      "mean total loss: 0.0019414698073645406\n",
      "Epoch 98: Validation Loss = 0.3754, Validation Accuracy = 0.9520\n",
      "----------epoch: 98------------\n",
      "mean total loss: 0.0019363766319822006\n",
      "Epoch 99: Validation Loss = 0.3700, Validation Accuracy = 0.9520\n",
      "----------epoch: 99------------\n",
      "mean total loss: 0.0019179114502861916\n",
      "Epoch 100: Validation Loss = 0.3760, Validation Accuracy = 0.9516\n"
     ]
    }
   ],
   "source": [
    "nn = FeedForwardNeuralNetwork(784, 30, 10)\n",
    "\n",
    "nn.train_sgd(X_train, Y_train_onehot, X_val, Y_val_onehot, 0.1, 1, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "# Convert to labels (0-9)\n",
    "y_pred = y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7, 2, 1, ..., 4, 5, 6]), array([7, 2, 1, ..., 4, 5, 6]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Print an output of the learning success per epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
